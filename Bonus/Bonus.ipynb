{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/20/0a/739426a81f7635b422fbe6cb8d1d99d1235579a6ac8024c13d743efa6847/transformers-4.36.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "     ---------------------------------------- 0.0/126.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/126.8 kB ? eta -:--:--\n",
      "     -------- ---------------------------- 30.7/126.8 kB 435.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 126.8/126.8 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting evaluate\n",
      "  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting accelerate\n",
      "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/f7/fc/c55e5a2da345c9a24aa2e1e0f60eb2ca290b6a41be82da03a6d4baec4f99/accelerate-0.25.0-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.19.3 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\thano\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thano\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/b3/34/65bb4b2d7908044963ebf614fe0fdb080773fc7030d7e39c8d3eddcd4257/PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/b8/ad/3398312096118c4e62a5827664e52a04d5068e84d04142dd4a0da8a567ae/regex-2023.10.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\thano\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/be/5f/2cc4f229bf85d90842f513be31a529595c10b8c8b8193c077230a8c17548/tokenizers-0.15.0-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/a5/dd/85bcfeab1451eeb24278575fa1b8f24bea7a99c5ff0bb34af63dfe20e772/safetensors-0.4.1-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp311-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Obtaining dependency information for tqdm>=4.27 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ----------------------------------- ---- 51.2/57.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 57.6/57.6 kB 764.0 kB/s eta 0:00:00\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Obtaining dependency information for pyarrow>=8.0.0 from https://files.pythonhosted.org/packages/43/3f/7bdf7dc3b3b0cfdcc60760e7880954ba99ccd0bc1e0df806f3dd61bc01cd/pyarrow-14.0.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pyarrow-14.0.2-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Obtaining dependency information for dill<0.3.8,>=0.3.0 from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\thano\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/b7/3a/74a609706ef4430fe6d041a3b8d209882c15440b695e373fe26d48c6f35c/xxhash-3.4.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/e7/41/96ac938770ba6e7d5ae1d8c9cafebac54b413549042c6260f0d0a6ec6622/multiprocess-0.70.15-py311-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets)\n",
      "  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/84/7a/70ca0dcffcb261d1e71590d1c93863f8b59415a52f610f75ee3e570e003c/aiohttp-3.9.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading aiohttp-3.9.1-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\thano\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (5.9.6)\n",
      "Collecting torch>=1.10.0 (from accelerate)\n",
      "  Obtaining dependency information for torch>=1.10.0 from https://files.pythonhosted.org/packages/e4/ae/2ad8820045b6631965750435f28583e80905b8273d57cf026163b51323ee/torch-2.1.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torch-2.1.2-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 0.0/61.2 kB ? eta -:--:--\n",
      "     --------------------------------- ------ 51.2/61.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 61.2/61.2 kB 652.8 kB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for yarl<2.0,>=1.0 from https://files.pythonhosted.org/packages/27/41/945ae9a80590e4fb0be166863c6e63d75e4b35789fa3a61ff1dbdcdc220f/yarl-1.9.4-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl.metadata (32 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/b3/21/c5aaffac47fd305d69df46cfbf118768cdf049a92ee6b0b5cb029d449dcf/frozenlist-1.4.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\thano\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\thano\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thano\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thano\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thano\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Collecting sympy (from torch>=1.10.0->accelerate)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "     ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.2/5.7 MB 7.0 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 0.5/5.7 MB 6.7 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.9/5.7 MB 6.9 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 1.2/5.7 MB 7.1 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 1.7/5.7 MB 7.6 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 2.1/5.7 MB 7.9 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 2.6/5.7 MB 8.4 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 3.2/5.7 MB 9.1 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 3.6/5.7 MB 8.8 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 4.0/5.7 MB 8.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 4.5/5.7 MB 9.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 5.2/5.7 MB 9.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.7/5.7 MB 9.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.7/5.7 MB 9.7 MB/s eta 0:00:00\n",
      "Collecting networkx (from torch>=1.10.0->accelerate)\n",
      "  Obtaining dependency information for networkx from https://files.pythonhosted.org/packages/d5/f0/8fbc882ca80cf077f1b246c0e3c3465f7f415439bdea6b899f6b19f61f70/networkx-3.2.1-py3-none-any.whl.metadata\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=1.10.0->accelerate)\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "     ---------------------------------------- 0.0/133.1 kB ? eta -:--:--\n",
      "     -------------------------------------- 133.1/133.1 kB 7.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\thano\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\thano\\appdata\\roaming\\python\\python311\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\thano\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\thano\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thano\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\thano\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.10.0->accelerate)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 536.2/536.2 kB 17.0 MB/s eta 0:00:00\n",
      "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "   ---------------------------------------- 0.0/8.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.7/8.2 MB 20.8 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.3/8.2 MB 16.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.8/8.2 MB 14.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.4/8.2 MB 14.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.0/8.2 MB 13.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.6/8.2 MB 13.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.2/8.2 MB 13.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.8/8.2 MB 13.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.4/8.2 MB 13.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.0/8.2 MB 13.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.6/8.2 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.2/8.2 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.7/8.2 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.2/8.2 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.2/8.2 MB 12.8 MB/s eta 0:00:00\n",
      "Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "   ---------------------------------------- 0.0/521.2 kB ? eta -:--:--\n",
      "   --------------------------------------  512.0/521.2 kB 16.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 521.2/521.2 kB 10.9 MB/s eta 0:00:00\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 84.1/84.1 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "   ---------------------------------------- 0.0/265.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 265.7/265.7 kB 17.0 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 115.3/115.3 kB ? eta 0:00:00\n",
      "Downloading aiohttp-3.9.1-cp311-cp311-win_amd64.whl (364 kB)\n",
      "   ---------------------------------------- 0.0/364.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 364.8/364.8 kB 23.6 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 311.7/311.7 kB 20.1 MB/s eta 0:00:00\n",
      "Downloading pyarrow-14.0.2-cp311-cp311-win_amd64.whl (24.6 MB)\n",
      "   ---------------------------------------- 0.0/24.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.7/24.6 MB 15.5 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 1.3/24.6 MB 14.1 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.9/24.6 MB 13.7 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 2.5/24.6 MB 13.5 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 3.1/24.6 MB 13.3 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 3.8/24.6 MB 13.3 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 4.4/24.6 MB 13.3 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.0/24.6 MB 13.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 5.5/24.6 MB 13.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 6.1/24.6 MB 13.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.7/24.6 MB 13.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 7.3/24.6 MB 13.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 7.9/24.6 MB 13.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.5/24.6 MB 13.0 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 9.1/24.6 MB 12.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.7/24.6 MB 12.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 10.3/24.6 MB 12.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.9/24.6 MB 12.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 11.5/24.6 MB 12.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 12.1/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.7/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.3/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 14.5/24.6 MB 12.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 15.1/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 15.7/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.3/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 16.9/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 17.5/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.1/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.7/24.6 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.3/24.6 MB 13.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.9/24.6 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.4/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.0/24.6 MB 12.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.2/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.8/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.4/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.6/24.6 MB 12.1 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "   ---------------------------------------- 0.0/144.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 144.7/144.7 kB ? eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp311-cp311-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 269.6/269.6 kB 16.2 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp311-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.5 kB ? eta -:--:--\n",
      "   ---------------------------------------  276.5/277.5 kB ? eta -:--:--\n",
      "   ---------------------------------------  276.5/277.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 277.5/277.5 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.7/2.2 MB 15.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.2 MB 14.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.9/2.2 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 13.9 MB/s eta 0:00:00\n",
      "Downloading torch-2.1.2-cp311-cp311-win_amd64.whl (192.3 MB)\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.6/192.3 MB 12.4 MB/s eta 0:00:16\n",
      "   ---------------------------------------- 1.2/192.3 MB 12.5 MB/s eta 0:00:16\n",
      "   ---------------------------------------- 1.8/192.3 MB 12.7 MB/s eta 0:00:15\n",
      "   ---------------------------------------- 2.4/192.3 MB 13.8 MB/s eta 0:00:14\n",
      "    --------------------------------------- 3.0/192.3 MB 12.8 MB/s eta 0:00:15\n",
      "    --------------------------------------- 3.6/192.3 MB 12.8 MB/s eta 0:00:15\n",
      "    --------------------------------------- 4.2/192.3 MB 12.9 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 4.8/192.3 MB 12.9 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 5.4/192.3 MB 12.9 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 6.1/192.3 MB 12.9 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 6.7/192.3 MB 12.9 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 7.3/192.3 MB 12.9 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 7.9/192.3 MB 12.9 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 8.5/192.3 MB 12.9 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 9.1/192.3 MB 12.9 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 9.7/192.3 MB 12.9 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 10.3/192.3 MB 12.8 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 10.9/192.3 MB 12.8 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 11.5/192.3 MB 12.9 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 12.0/192.3 MB 12.8 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 12.6/192.3 MB 13.1 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 13.2/192.3 MB 13.1 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 13.8/192.3 MB 13.1 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 14.4/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 15.0/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 15.6/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 16.2/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 16.8/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 17.4/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 18.0/192.3 MB 12.9 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 18.6/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 19.2/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 19.8/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 20.4/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 20.9/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 21.5/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 22.1/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 22.8/192.3 MB 12.9 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 23.3/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 23.9/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 24.5/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 25.1/192.3 MB 12.8 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 25.7/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 26.3/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 26.9/192.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 27.5/192.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 28.1/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 28.7/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 29.3/192.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 29.9/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 30.5/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 31.1/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 31.7/192.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 32.3/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 32.8/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 33.4/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 34.0/192.3 MB 12.9 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 34.7/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 35.2/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 35.8/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 36.4/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 37.0/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 37.6/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 38.2/192.3 MB 12.8 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 38.8/192.3 MB 12.9 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 39.4/192.3 MB 12.8 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 40.0/192.3 MB 12.8 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 40.6/192.3 MB 12.8 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 41.2/192.3 MB 12.8 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 41.7/192.3 MB 12.8 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 41.7/192.3 MB 12.8 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 42.8/192.3 MB 12.8 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 43.4/192.3 MB 12.6 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 44.0/192.3 MB 12.6 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 44.6/192.3 MB 12.8 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 45.2/192.3 MB 12.6 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 45.8/192.3 MB 12.8 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 46.4/192.3 MB 12.8 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 47.0/192.3 MB 12.8 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 47.6/192.3 MB 12.8 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 48.2/192.3 MB 12.8 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 48.8/192.3 MB 12.8 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 49.4/192.3 MB 12.8 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 50.0/192.3 MB 12.6 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 50.6/192.3 MB 12.6 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 51.2/192.3 MB 12.6 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 51.7/192.3 MB 12.6 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 52.3/192.3 MB 13.6 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 52.9/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 53.5/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 54.1/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 54.7/192.3 MB 12.9 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 55.3/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 55.9/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 56.5/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 57.2/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 57.8/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 58.4/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 58.9/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 59.5/192.3 MB 12.9 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 60.1/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 60.7/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 61.3/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 61.9/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 62.5/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 63.1/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 63.7/192.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 64.3/192.3 MB 12.9 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 64.9/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 65.5/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 66.1/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 66.7/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 67.4/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 67.9/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 68.5/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 69.1/192.3 MB 12.9 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 69.7/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 70.3/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 70.9/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 71.5/192.3 MB 13.1 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 72.1/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 72.7/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 73.3/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 73.9/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 74.5/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 75.1/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 75.7/192.3 MB 12.9 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 76.3/192.3 MB 12.8 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 76.9/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 77.5/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 78.0/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 78.6/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 79.2/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 79.8/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 80.4/192.3 MB 12.9 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 81.0/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 81.6/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 82.2/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 82.8/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 83.4/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 84.0/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 84.6/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 85.2/192.3 MB 12.9 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 85.8/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 86.4/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 87.0/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 87.6/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 88.2/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 88.8/192.3 MB 12.8 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 89.4/192.3 MB 13.1 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 90.0/192.3 MB 13.1 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 90.6/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 91.2/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 91.8/192.3 MB 12.9 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 92.4/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 92.9/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 93.6/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 94.2/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 94.7/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 95.3/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 95.9/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 96.5/192.3 MB 12.9 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 97.1/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 97.7/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 98.3/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 98.9/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 99.5/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 100.1/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 100.7/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 101.3/192.3 MB 12.9 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 101.9/192.3 MB 12.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 102.5/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 103.1/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 103.7/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 104.3/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 104.9/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 105.5/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 106.1/192.3 MB 12.9 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 106.7/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 107.3/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 107.9/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 108.5/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 109.1/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 109.7/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 110.3/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 110.9/192.3 MB 12.9 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 111.5/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 112.1/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 112.7/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 113.3/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 113.9/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 114.5/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 115.1/192.3 MB 12.8 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 115.7/192.3 MB 12.9 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 116.3/192.3 MB 13.1 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 116.9/192.3 MB 12.8 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 117.5/192.3 MB 12.8 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 118.1/192.3 MB 13.1 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 118.7/192.3 MB 13.1 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 119.3/192.3 MB 13.1 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 119.9/192.3 MB 13.1 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 120.5/192.3 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 121.1/192.3 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 121.7/192.3 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 122.0/192.3 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 122.0/192.3 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 123.3/192.3 MB 12.6 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 123.9/192.3 MB 12.6 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 124.5/192.3 MB 12.6 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 125.1/192.3 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 125.7/192.3 MB 12.6 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 126.4/192.3 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 127.0/192.3 MB 12.9 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 127.5/192.3 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 128.2/192.3 MB 12.8 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 128.8/192.3 MB 12.8 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 129.4/192.3 MB 12.8 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 130.0/192.3 MB 12.8 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 130.5/192.3 MB 12.6 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 131.1/192.3 MB 12.6 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 131.7/192.3 MB 12.6 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 132.3/192.3 MB 13.9 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 132.9/192.3 MB 13.1 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 133.5/192.3 MB 12.8 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 134.1/192.3 MB 12.8 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 134.7/192.3 MB 12.8 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 135.3/192.3 MB 12.8 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 135.9/192.3 MB 12.8 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 136.5/192.3 MB 12.9 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 137.1/192.3 MB 12.8 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 137.7/192.3 MB 12.8 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 138.3/192.3 MB 13.1 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 138.9/192.3 MB 12.8 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 139.5/192.3 MB 12.8 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 140.0/192.3 MB 12.8 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 140.6/192.3 MB 12.8 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 141.2/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 141.8/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 142.4/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 143.0/192.3 MB 12.9 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 143.6/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 144.2/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 144.8/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 145.4/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 146.0/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 146.6/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 147.2/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 147.8/192.3 MB 12.9 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 148.4/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 149.0/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 149.6/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 150.2/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 150.8/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 151.4/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 152.0/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 152.6/192.3 MB 12.9 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 153.2/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 153.8/192.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 154.4/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 155.0/192.3 MB 13.1 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 155.5/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 156.1/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 156.8/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 157.3/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 157.9/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 158.5/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 159.1/192.3 MB 12.9 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 159.7/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 160.3/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 160.9/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 161.5/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 162.1/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 162.7/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 163.3/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 163.9/192.3 MB 12.9 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 164.5/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 165.1/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 165.7/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 166.3/192.3 MB 12.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 166.9/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 167.5/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 168.1/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 168.7/192.3 MB 12.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 169.2/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 169.8/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 170.4/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 171.0/192.3 MB 13.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 171.6/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 172.2/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 172.8/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 173.4/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 174.0/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 174.6/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 175.2/192.3 MB 12.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 175.7/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 176.3/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 176.9/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 177.6/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 178.2/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 178.7/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 179.3/192.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 179.9/192.3 MB 12.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 180.5/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 181.1/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 181.7/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 182.3/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 182.9/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 183.5/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 184.1/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 184.7/192.3 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 185.3/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 185.9/192.3 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 186.4/192.3 MB 12.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 187.0/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  187.6/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  188.2/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  188.8/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  189.4/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  190.0/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  190.6/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  191.2/192.3 MB 12.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  191.8/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 192.3/192.3 MB 8.6 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB ? eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "   ---------------------------------------- 0.0/135.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 135.4/135.4 kB ? eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl (29 kB)\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.5/50.5 kB ? eta 0:00:00\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 166.4/166.4 kB ? eta 0:00:00\n",
      "Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.7/76.7 kB ? eta 0:00:00\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.7/1.6 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.3/1.6 MB 13.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 13.1 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, xxhash, tqdm, sympy, safetensors, regex, pyyaml, pyarrow-hotfix, pyarrow, networkx, multidict, jinja2, fsspec, frozenlist, filelock, dill, attrs, yarl, torch, responses, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, accelerate, transformers, datasets, evaluate\n",
      "Successfully installed accelerate-0.25.0 aiohttp-3.9.1 aiosignal-1.3.1 attrs-23.1.0 datasets-2.15.0 dill-0.3.7 evaluate-0.4.1 filelock-3.13.1 frozenlist-1.4.1 fsspec-2023.10.0 huggingface-hub-0.19.4 jinja2-3.1.2 mpmath-1.3.0 multidict-6.0.4 multiprocess-0.70.15 networkx-3.2.1 pyarrow-14.0.2 pyarrow-hotfix-0.6 pyyaml-6.0.1 regex-2023.10.3 responses-0.18.0 safetensors-0.4.1 sympy-1.12 tokenizers-0.15.0 torch-2.1.2 tqdm-4.66.1 transformers-4.36.2 xxhash-3.4.1 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU availability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text  label\n",
      "0           0  #anywere &#8220;@TheCooleyShow: LA = palm tree...      1\n",
      "1           1  RT @OfficialA1King: The face you make when you...      1\n",
      "2           2             bitch get off my twitter hoe &#128074;      1\n",
      "3           3    I can taste loud n pussy on my tongue &#128541;      1\n",
      "4           4  Diabetes galore &#128514;&#128514;&#128514;&#1...      2\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = 'train.csv'  # Update the path if necessary\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Dataset class for handling the Twitter comments. This involves tokenizing the text and preparing it in a format suitable for BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterCommentsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(data, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len = 256  # You can adjust this depending on the average length of the tweets\n",
    "\n",
    "train_dataset = TwitterCommentsDataset(\n",
    "    texts=df_train.text.to_numpy(),\n",
    "    labels=df_train.label.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    ")\n",
    "\n",
    "val_dataset = TwitterCommentsDataset(\n",
    "    texts=df_val.text.to_numpy(),\n",
    "    labels=df_val.label.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 440M/440M [00:34<00:00, 12.6MB/s] \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(data.label.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/3348 [01:56<10:42:57, 11.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9403, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/3348 [04:02<11:29:29, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.876, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 27/3348 [05:25<10:43:39, 11.63s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m      4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m      5\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1860\u001b[0m ):\n\u001b[0;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2735\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2738\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2758\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2757\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2758\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2759\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2760\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1578\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:355\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    352\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[1;32mc:\\Users\\thano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:1826\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1822\u001b[0m         ret \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[1;32m-> 1826\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(\u001b[38;5;28minput\u001b[39m: Tensor, dim: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, _stacklevel: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, dtype: Optional[DType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies a softmax function.\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m \n\u001b[0;32m   1829\u001b[0m \u001b[38;5;124;03m    Softmax is defined as:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1849\u001b[0m \n\u001b[0;32m   1850\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evuluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
